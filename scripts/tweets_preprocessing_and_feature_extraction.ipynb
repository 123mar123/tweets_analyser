{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfquD4lQSXmb"
   },
   "source": [
    "## 1. Call function _transform(tweets__raw)_ -> _tweets__raw_ is dataframe with tweets' data from file 'tweets.csv'.\n",
    "## 2. Function _transform(tweets__raw)_ returns dataframe, which contains rows corresponding consecutive tweets and colums with extracted features.\n",
    "## 3. Features list:\n",
    "* 'author',\n",
    "* 'encoded_tweet_long' -> all words in tweet represented as numbers (list of integers),\n",
    "* 'encoded_tweet_short' -> words in tweet without stopwords (taken from nltk package) represented as numbers (list of integers),\n",
    "* 'letters_nr',\n",
    "* 'urls_nr',\n",
    "* 'hashtag_nr',\n",
    "* 'mentioned_nr' -> e.g. @SelenaGomez,\n",
    "* 'exclamations_nr',\n",
    "* 'emojis_nr',\n",
    "* 'perc_of_upper' -> percentage of upper case letters,\n",
    "* 'words_nr' -> number of all words in tweet,\n",
    "* 'average_word_len',\n",
    "* 'std_dev_word_len' -> standard deviation of word's length,\n",
    "* 'min_word_len',\n",
    "* 'max_word_len',\n",
    "* 'time' -> time of tweet posting represented as number of minutes elapsed from midnight (integer),\n",
    "* 'weekday' -> weekday represented as numeric value e.g. Monday = 1 (inetger)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DsycaN4Ekq7k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /Users/Kasia/anaconda3/lib/python3.7/site-packages (0.5.4)\n",
      "Requirement already satisfied: keras in /Users/Kasia/anaconda3/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/Kasia/anaconda3/lib/python3.7/site-packages (from keras) (1.16.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/Kasia/anaconda3/lib/python3.7/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/Kasia/anaconda3/lib/python3.7/site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/Kasia/anaconda3/lib/python3.7/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/Kasia/anaconda3/lib/python3.7/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: h5py in /Users/Kasia/anaconda3/lib/python3.7/site-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: pyyaml in /Users/Kasia/anaconda3/lib/python3.7/site-packages (from keras) (3.13)\n",
      "Requirement already satisfied: regex in /Users/Kasia/anaconda3/lib/python3.7/site-packages (2020.1.8)\n",
      "WARNING:tensorflow:From /Users/Kasia/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/compat.py:175: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#Imports and installations\n",
    "!pip install emoji\n",
    "!pip install keras\n",
    "!pip install regex\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWImK2D9Zwvr"
   },
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import datetime\n",
    "import statistics as stat\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import emoji\n",
    "import regex\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9m-UGHEJmkP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Kasia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Kasia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Additional downloads\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ss8TdzszZN42"
   },
   "outputs": [],
   "source": [
    "#Download dataset (use in Colab)\n",
    "#%%capture\n",
    "#if not os.path.isfile('tweets.csv'):\n",
    "    #!wget 'https://drive.google.com/uc?export=download&id=17F1luxwaaE4vrhlFsHbOFjSoYhsThuAJ' -O tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "EvPbySpkaIsB",
    "outputId": "76765fc0-2ea4-4f84-e284-73de5f298bd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets and their features:  (52542, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>country</th>\n",
       "      <th>date_time</th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>number_of_likes</th>\n",
       "      <th>number_of_shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/01/2017 19:52</td>\n",
       "      <td>8.196330e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7900</td>\n",
       "      <td>3472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>@barackobama Thank you for your incredible gra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/01/2017 08:38</td>\n",
       "      <td>8.191010e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3689</td>\n",
       "      <td>1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>Life goals. https://t.co/XIn1qKMKQl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/01/2017 02:52</td>\n",
       "      <td>8.190140e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10341</td>\n",
       "      <td>2387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>Me right now üôèüèª https://t.co/gW55C1wrwd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/01/2017 02:44</td>\n",
       "      <td>8.190120e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10774</td>\n",
       "      <td>2458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/01/2017 05:22</td>\n",
       "      <td>8.186890e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17620</td>\n",
       "      <td>4655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      author                                            content country  \\\n",
       "0  katyperry  Is history repeating itself...?#DONTNORMALIZEH...     NaN   \n",
       "1  katyperry  @barackobama Thank you for your incredible gra...     NaN   \n",
       "2  katyperry                Life goals. https://t.co/XIn1qKMKQl     NaN   \n",
       "3  katyperry            Me right now üôèüèª https://t.co/gW55C1wrwd     NaN   \n",
       "4  katyperry  SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...     NaN   \n",
       "\n",
       "          date_time            id language  latitude  longitude  \\\n",
       "0  12/01/2017 19:52  8.196330e+17       en       NaN        NaN   \n",
       "1  11/01/2017 08:38  8.191010e+17       en       NaN        NaN   \n",
       "2  11/01/2017 02:52  8.190140e+17       en       NaN        NaN   \n",
       "3  11/01/2017 02:44  8.190120e+17       en       NaN        NaN   \n",
       "4  10/01/2017 05:22  8.186890e+17       en       NaN        NaN   \n",
       "\n",
       "   number_of_likes  number_of_shares  \n",
       "0             7900              3472  \n",
       "1             3689              1380  \n",
       "2            10341              2387  \n",
       "3            10774              2458  \n",
       "4            17620              4655  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create data frame\n",
    "tweets_raw = pd.read_csv('../data/tweets.csv')\n",
    "print(\"Number of tweets and their features: \", tweets_raw.shape)\n",
    "tweets_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "wl9T9LWHbNJ7",
    "outputId": "7f4e0df3-d0c0-4157-e507-6dcc6402ab49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52542 entries, 0 to 52541\n",
      "Data columns (total 10 columns):\n",
      "author              52542 non-null object\n",
      "content             52542 non-null object\n",
      "country             36 non-null object\n",
      "date_time           52542 non-null object\n",
      "id                  52542 non-null float64\n",
      "language            52542 non-null object\n",
      "latitude            1 non-null float64\n",
      "longitude           1 non-null float64\n",
      "number_of_likes     52542 non-null int64\n",
      "number_of_shares    52542 non-null int64\n",
      "dtypes: float64(3), int64(2), object(5)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "#Show general info\n",
    "tweets_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "AMks4mugcyIP",
    "outputId": "d560efe6-ca7b-4933-9ad2-ee58471d6c5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TheEllenShow     3147\n",
       "jimmyfallon      3123\n",
       "ArianaGrande     3104\n",
       "YouTube          3077\n",
       "KimKardashian    2939\n",
       "katyperry        2924\n",
       "selenagomez      2913\n",
       "rihanna          2877\n",
       "BarackObama      2863\n",
       "britneyspears    2776\n",
       "instagram        2577\n",
       "shakira          2530\n",
       "Cristiano        2507\n",
       "jtimberlake      2478\n",
       "ladygaga         2329\n",
       "Twitter          2290\n",
       "ddlovato         2217\n",
       "taylorswift13    2029\n",
       "justinbieber     2000\n",
       "cnnbrk           1842\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tweets per person \n",
    "tweets_raw['author'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Jq9rwQSZQuPD",
    "outputId": "0ea7d057-084c-445c-fe42-ae5906f559b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOU GOT THIS @HillaryClinton #DEBATES https://t.co/IZgi0yL9T2\n"
     ]
    }
   ],
   "source": [
    "# 149 contains: #, @ i https\n",
    "# 198 contains two #\n",
    "# 114 contains two https\n",
    "\n",
    "tweet_nr = 149\n",
    "tweet = tweets_raw['content'][tweet_nr]\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gxMlbj2afaEW"
   },
   "outputs": [],
   "source": [
    "def find_urls(text):\n",
    "    \"finds all URLs in the given text and returns the list of them\"\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-3MCjj1k8PP"
   },
   "outputs": [],
   "source": [
    "def nr_of_urls(text):\n",
    "    return len(find_urls(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUQpvvFQ0QRd"
   },
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    return re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5XJ2UbIgmCq"
   },
   "outputs": [],
   "source": [
    "def find_mentioned(text):\n",
    "    \"finds all mentions in the given text and returns the list of them. Ommits emails.\"\n",
    "    # this line removes email adresses\n",
    "    text = re.sub(\"[\\w]+@[\\w]+\\.[c][o][m]\", \"\", text)\n",
    "    mentions = re.findall('@([a-zA-Z0-9]{1,15})', text)\n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3Mf9diY0aT5"
   },
   "outputs": [],
   "source": [
    "def count_mentioned(text):\n",
    "    return len(find_mentioned(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJy5sdwz05EL"
   },
   "outputs": [],
   "source": [
    "def remove_mentions_and_emails(text):\n",
    "    text = re.sub(\"[\\w]+@[\\w]+\\.[c][o][m]\", \"\", text)\n",
    "    text = re.sub(\"@([a-zA-Z0-9]{1,15})\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FSBP5PMVl0M-"
   },
   "outputs": [],
   "source": [
    "def find_hashtags(text):\n",
    "    \"finds all hashtags in the given text and returns the list of them. Will catch other #.\"\n",
    "    #return list(part[1:] for part in text.split() if part.startswith('#')) # this version won't catch hashtags with no blank spaces before them\n",
    "    return re.findall(r\"#(\\w+)\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkrSIT9vnjdJ"
   },
   "outputs": [],
   "source": [
    "def count_hashtags(text):\n",
    "    return len(find_hashtags(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8DgnWTUV1nu9"
   },
   "outputs": [],
   "source": [
    "def remove_hashtags(text):\n",
    "    return re.sub(r\"#(\\w+)\", '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C5pEpl3nprv4"
   },
   "outputs": [],
   "source": [
    "def count_letters(text):\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VQtOkWN2qonR"
   },
   "outputs": [],
   "source": [
    "def count_exclamation(text):\n",
    "    return text.count('!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WlyDYXu3HWBi"
   },
   "outputs": [],
   "source": [
    "def percent_of_upper(text):\n",
    "    upper = len(re.findall(r'[A-Z]', text))\n",
    "    return upper / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eHLJDuEHrmga"
   },
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "    \"finds all emoji in the given text and returns the list of them\"\n",
    "    clean_text = regex.findall(r'\\X', text)\n",
    "    return [word for word in clean_text if any(char in emoji.UNICODE_EMOJI for char in word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qb5eis6dl89Y"
   },
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    return text.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kp2XABpiecxd"
   },
   "outputs": [],
   "source": [
    "punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hMuc6SBrIb3B"
   },
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ngUkxcgJb2c"
   },
   "outputs": [],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ey-D6z9CL3s6"
   },
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Rsfgvw3MLff"
   },
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z5DMLtOpKM8i"
   },
   "outputs": [],
   "source": [
    "def leave_words(text):\n",
    "    \"takes text as list of words. returns list deleting strange things ;)\"\n",
    "    return [word for word in text if re.search('[a-zA-Z]', word) is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MAmR54iXywoJ"
   },
   "outputs": [],
   "source": [
    "def calculate_word_length_list(words_list):\n",
    "    return list(map(len, words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ATqXnJwUxMwr"
   },
   "outputs": [],
   "source": [
    "def calculate_average_word_length(words_list):\n",
    "    words_len_list = calculate_word_length_list(words_list)\n",
    "    return stat.mean(words_len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFu-KJ64zEOH"
   },
   "outputs": [],
   "source": [
    "def calculate_std_deviation_word_length(words_list):\n",
    "    words_len_list = calculate_word_length_list(words_list)\n",
    "    if len(words_len_list) == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return stat.stdev(words_len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oxUX7nTPys8i"
   },
   "outputs": [],
   "source": [
    "def calculate_max_word_length(words_list):\n",
    "    words_len_list = calculate_word_length_list(words_list)\n",
    "    return max(words_len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3tMpoCLy-qm"
   },
   "outputs": [],
   "source": [
    "def calculate_min_word_length(words_list):\n",
    "    words_len_list = calculate_word_length_list(words_list)\n",
    "    return min(words_len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jl_3IDfruAnt"
   },
   "outputs": [],
   "source": [
    "def split_date_time(date_time):\n",
    "    \"takes date as string. return list in form: [day, month, year, hour, minute], every element is converted to int\"\n",
    "    date_time_split = re.sub(\"[^\\w]\", \" \",  date_time).split()\n",
    "    return list(map(int, date_time_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nfc3eiPXt4qb"
   },
   "outputs": [],
   "source": [
    "def calculate_time(date_time):\n",
    "    \"takes date as string. returns time in minutes elapsed from midnight\"\n",
    "    date_time_split = split_date_time(date_time)\n",
    "    return date_time_split[3]*60 + date_time_split[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6EFUKhUujkX"
   },
   "outputs": [],
   "source": [
    "def calculate_weekday(date_time):\n",
    "    \"takes date as string. returns weekday\"\n",
    "    date_time_split = split_date_time(date_time)\n",
    "    return datetime.date(date_time_split[2],date_time_split[1],date_time_split[0]).weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2jISNpipH34A"
   },
   "outputs": [],
   "source": [
    "def transform_for_words_long_coding(text):\n",
    "    t = remove_emojis(text)\n",
    "    t = remove_urls(t)\n",
    "    t = remove_hashtags(t)\n",
    "    t = remove_mentions_and_emails(t)\n",
    "    t = remove_punct(t)\n",
    "    t = tokenization(t.lower())\n",
    "    t = leave_words(t)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMFf8XdOnJv3"
   },
   "outputs": [],
   "source": [
    "def transform_for_words_short_coding(text):\n",
    "    t = transform_for_words_long_coding(text)\n",
    "    t = remove_stopwords(t)\n",
    "    t = stemming(t)\n",
    "    t = lemmatizer(t)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SM05Xj0GGMi9"
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(texts, type):\n",
    "    transformed_texts = []\n",
    "    for t in texts:\n",
    "        if type == \"long\":\n",
    "            transformed_texts.append(transform_for_words_long_coding(t))\n",
    "        elif type == \"short\":\n",
    "            transformed_texts.append(transform_for_words_short_coding(t))\n",
    "        else:\n",
    "            print(\"Incorrect tokenizer type. Must be 'long' or 'short'.\")\n",
    "    # create the tokenizer\n",
    "    tok = Tokenizer()\n",
    "    # fit the tokenizer on words\n",
    "    tok.fit_on_texts(transformed_texts)\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_codes(texts, type):\n",
    "    transformed_texts = []\n",
    "    for t in texts:\n",
    "        if type == \"long\":\n",
    "            transformed_texts.append(transform_for_words_long_coding(t))\n",
    "        elif type == \"short\":\n",
    "            transformed_texts.append(transform_for_words_short_coding(t))\n",
    "        else:\n",
    "            print(\"Incorrect tokenizer type. Must be 'long' or 'short'.\")\n",
    "    char_list = [c for t in transformed_texts for s in t for c in s]\n",
    "    unique_chars = sorted(set(char_list))\n",
    "    return {u:i for i, u in enumerate(unique_chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_words_nr(texts, type):\n",
    "    if type == \"long\":\n",
    "        return max([len( transform_for_words_long_coding(t) ) for t in texts])\n",
    "    elif type == \"short\":\n",
    "        return max([len( transform_for_words_short_coding(t) ) for t in texts])\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(list_to_extend, basic_len, extended_len):\n",
    "    list_to_extend.extend([0]*(extended_len - basic_len))\n",
    "    return list_to_extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rglRtMp28xDY"
   },
   "outputs": [],
   "source": [
    "def transform_row(text, author, nr_of_shares, nr_of_likes, date_time, tokenizer_long, tokenizer_short, char_codes, max_words_nr_long, max_words_nr_short, padding):\n",
    "    \"takes tweet text, nr of shares and nr of likes and returns extracted features\"\n",
    "    nr_of_letters = count_letters(text)\n",
    "    urls_list = find_urls(text)\n",
    "    urls_nr = len(urls_list)\n",
    "    hashtag_list = find_hashtags(text)\n",
    "    hashtag_nr = len(hashtag_list)\n",
    "    mentioned_list = find_mentioned(text)\n",
    "    mentioned_nr = len(mentioned_list)\n",
    "    exclamations_nr = count_exclamation(text)\n",
    "    emojis_list = extract_emojis(text)\n",
    "    emojis_nr = len(emojis_list)\n",
    "    t = remove_emojis(text)\n",
    "    t = remove_urls(t)\n",
    "    t = remove_hashtags(t)\n",
    "    t = remove_mentions_and_emails(t)\n",
    "    t = remove_punct(t)\n",
    "    perc_of_upper = percent_of_upper(t)\n",
    "    t = tokenization(t.lower())\n",
    "    t = leave_words(t)\n",
    "    nr_of_words = len(t)\n",
    "    average_word_len = calculate_average_word_length(t)\n",
    "    std_dev_word_len = calculate_std_deviation_word_length(t)\n",
    "    min_word_len = calculate_min_word_length(t)\n",
    "    max_word_len = calculate_max_word_length(t)\n",
    "    # first character and words coding\n",
    "    encoded_tweet_long = tokenizer_long.texts_to_sequences([t])[0]\n",
    "    if padding and len(encoded_tweet_long) < max_words_nr_long:\n",
    "        encoded_tweet_long = add_padding(encoded_tweet_long, len(encoded_tweet_long), max_words_nr_long)\n",
    "    encoded_tweet_chars = [char_codes[c] for s in t for c in s]\n",
    "    t = remove_stopwords(t)\n",
    "    t = stemming(t)\n",
    "    t = lemmatizer(t)\n",
    "    # second character and words coding\n",
    "    encoded_tweet_short = tokenizer_short.texts_to_sequences([t])[0]\n",
    "    if padding and len(encoded_tweet_short) < max_words_nr_short:\n",
    "        encoded_tweet_short = add_padding(encoded_tweet_short, len(encoded_tweet_short), max_words_nr_short)\n",
    "    time = calculate_time(date_time)\n",
    "    weekday = calculate_weekday(date_time)\n",
    "    return [author, encoded_tweet_long, encoded_tweet_short, encoded_tweet_chars, nr_of_letters, urls_nr, hashtag_nr, mentioned_nr, \\\n",
    "            exclamations_nr, emojis_nr, perc_of_upper, nr_of_words, average_word_len, \\\n",
    "            std_dev_word_len, min_word_len, max_word_len, time, weekday]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POAoDnVy-_f1"
   },
   "outputs": [],
   "source": [
    "def transform(tweets_raw):\n",
    "    \"napisaƒá funkcjƒô, kt√≥ra najpierw tworzy ten s≈Çownik (word_index), potem korzystaƒá z niego dla kolejnych tweet√≥w\"\n",
    "    tokenizer_long = create_tokenizer(tweets_raw['content'][:20], \"long\")\n",
    "    tokenizer_short = create_tokenizer(tweets_raw['content'][:20], \"short\")\n",
    "    char_codes = create_char_codes(tweets_raw['content'][:20], \"long\")\n",
    "    max_words_nr_long = get_max_words_nr(tweets_raw['content'][:20], \"long\")\n",
    "    max_words_nr_short = get_max_words_nr(tweets_raw['content'][:20], \"short\")\n",
    "    #print(tokenizer_short.word_index)\n",
    "    features_list = [transform_row(row['content'], row['author'], row['number_of_shares'], row['number_of_likes'], row['date_time'], tokenizer_long, tokenizer_short, char_codes, max_words_nr_long, max_words_nr_short, True) \\\n",
    "        for index, row in tweets_raw[:20].iterrows()]\n",
    "    return pd.DataFrame(features_list, columns = ['author', 'encoded_tweet_long', 'encoded_tweet_short', 'encoded_tweet_chars', 'nr_of_letters', 'urls_nr', \\\n",
    "                                                  'hashtag_nr', 'mentioned_nr', 'exclamations_nr', 'emojis_nr', 'perc_of_upper', \\\n",
    "                                                  'nr_of_words', 'average_word_len', 'std_dev_word_len', 'min_word_len', 'max_word_len', \\\n",
    "                                                  'time', 'weekday']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "EW6_MVWiNfKS",
    "outputId": "ff73df43-69f7-4ce7-e904-cf82cb42e783"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       author                                 encoded_tweet_long  \\\n",
      "0   katyperry  [9, 20, 21, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "1   katyperry  [23, 10, 2, 11, 24, 25, 12, 26, 3, 2, 27, 28, ...   \n",
      "2   katyperry  [30, 31, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
      "3   katyperry  [32, 33, 34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "4   katyperry  [13, 35, 36, 37, 2, 38, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
      "5   katyperry  [14, 39, 40, 41, 42, 43, 44, 45, 15, 46, 0, 0,...   \n",
      "6   katyperry  [47, 48, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
      "7   katyperry  [49, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
      "8   katyperry  [51, 1, 52, 53, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "9   katyperry  [13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
      "10  katyperry  [14, 54, 55, 16, 3, 56, 1, 57, 58, 15, 4, 59, ...   \n",
      "11  katyperry  [60, 61, 62, 1, 63, 64, 5, 65, 66, 67, 0, 0, 0...   \n",
      "12  katyperry  [5, 68, 69, 70, 71, 6, 7, 72, 17, 73, 0, 0, 0,...   \n",
      "13  katyperry  [5, 11, 17, 74, 9, 75, 76, 77, 0, 0, 0, 0, 0, ...   \n",
      "14  katyperry  [6, 10, 78, 79, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "15  katyperry  [18, 80, 81, 3, 8, 82, 1, 83, 19, 84, 3, 6, 7,...   \n",
      "16  katyperry  [16, 4, 89, 90, 91, 92, 93, 0, 0, 0, 0, 0, 0, ...   \n",
      "17  katyperry  [2, 94, 95, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
      "18  katyperry  [8, 96, 97, 1, 98, 18, 99, 7, 100, 101, 102, 1...   \n",
      "19  katyperry  [8, 108, 109, 110, 111, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
      "\n",
      "                     encoded_tweet_short  \\\n",
      "0            [5, 6, 0, 0, 0, 0, 0, 0, 0]   \n",
      "1          [7, 8, 9, 10, 11, 0, 0, 0, 0]   \n",
      "2          [12, 13, 0, 0, 0, 0, 0, 0, 0]   \n",
      "3           [14, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "4           [2, 15, 0, 0, 0, 0, 0, 0, 0]   \n",
      "5    [3, 16, 17, 18, 19, 20, 21, 22, 23]   \n",
      "6          [24, 25, 0, 0, 0, 0, 0, 0, 0]   \n",
      "7          [26, 27, 0, 0, 0, 0, 0, 0, 0]   \n",
      "8         [28, 29, 30, 0, 0, 0, 0, 0, 0]   \n",
      "9            [2, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "10      [3, 1, 31, 4, 32, 33, 34, 35, 0]   \n",
      "11    [36, 37, 38, 39, 40, 41, 42, 0, 0]   \n",
      "12      [43, 44, 45, 46, 1, 47, 0, 0, 0]   \n",
      "13       [1, 48, 49, 50, 51, 0, 0, 0, 0]   \n",
      "14          [52, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "15   [53, 54, 55, 56, 57, 58, 59, 60, 0]   \n",
      "16       [4, 61, 62, 63, 64, 0, 0, 0, 0]   \n",
      "17         [65, 66, 0, 0, 0, 0, 0, 0, 0]   \n",
      "18  [67, 68, 69, 70, 71, 72, 73, 74, 75]   \n",
      "19        [76, 77, 78, 0, 0, 0, 0, 0, 0]   \n",
      "\n",
      "                                  encoded_tweet_chars  nr_of_letters  urls_nr  \\\n",
      "0   [8, 17, 7, 8, 17, 18, 14, 16, 23, 16, 4, 15, 4...             73        1   \n",
      "1   [18, 7, 0, 13, 10, 23, 14, 19, 5, 14, 16, 23, ...            116        1   \n",
      "2                     [11, 8, 5, 4, 6, 14, 0, 11, 17]             35        1   \n",
      "3                [12, 4, 16, 8, 6, 7, 18, 13, 14, 21]             39        1   \n",
      "4   [17, 8, 17, 18, 4, 16, 17, 0, 16, 4, 3, 14, 8,...             67        1   \n",
      "5   [7, 0, 15, 15, 23, 18, 7, 6, 12, 0, 11, 0, 2, ...             97        1   \n",
      "6               [10, 23, 14, 18, 14, 9, 0, 15, 0, 13]             48        1   \n",
      "7   [17, 0, 13, 16, 8, 14, 15, 19, 16, 14, 11, 0, ...             44        1   \n",
      "8   [16, 4, 17, 14, 11, 19, 18, 8, 14, 13, 18, 14,...             40        0   \n",
      "9                          [17, 8, 17, 18, 4, 16, 17]             32        1   \n",
      "10  [7, 0, 15, 15, 23, 7, 14, 11, 8, 3, 0, 23, 17,...             69        0   \n",
      "11  [3, 0, 12, 13, 8, 18, 17, 7, 0, 16, 3, 18, 14,...             61        0   \n",
      "12  [21, 7, 4, 13, 12, 23, 21, 7, 14, 11, 4, 5, 0,...             80        1   \n",
      "13  [21, 7, 4, 13, 23, 14, 19, 16, 7, 14, 11, 8, 3...            102        2   \n",
      "14  [7, 0, 20, 4, 23, 14, 19, 1, 4, 4, 13, 7, 0, 2...             37        0   \n",
      "15  [18, 7, 8, 17, 21, 4, 4, 10, 17, 0, 13, 18, 0,...            120        1   \n",
      "16  [11, 14, 20, 4, 18, 7, 4, 12, 19, 17, 8, 2, 18...             63        1   \n",
      "17               [5, 14, 16, 16, 4, 0, 11, 18, 7, 14]             36        1   \n",
      "18  [8, 2, 0, 13, 18, 21, 0, 8, 18, 18, 14, 16, 4,...            108        1   \n",
      "19  [8, 21, 0, 13, 13, 0, 0, 3, 14, 15, 18, 7, 8, ...             47        1   \n",
      "\n",
      "    hashtag_nr  mentioned_nr  exclamations_nr  emojis_nr  perc_of_upper  \\\n",
      "0            1             0                0          0       0.035714   \n",
      "1            0             1                0          0       0.012500   \n",
      "2            0             0                0          0       0.090909   \n",
      "3            0             0                0          1       0.071429   \n",
      "4            0             0                1          3       0.805556   \n",
      "5            1             0                1          1       0.181818   \n",
      "6            0             0                0          0       0.117647   \n",
      "7            0             0                0          1       0.111111   \n",
      "8            0             0                1          0       0.000000   \n",
      "9            0             0                0          0       0.000000   \n",
      "10           0             0                1          1       0.044776   \n",
      "11           0             1                0          0       0.019608   \n",
      "12           0             0                0          1       0.018182   \n",
      "13           0             0                0          1       0.018519   \n",
      "14           0             1                0          0       0.047619   \n",
      "15           0             0                0          0       0.031915   \n",
      "16           0             0                1          0       0.128205   \n",
      "17           0             0                0          0       0.076923   \n",
      "18           0             0                0          0       0.012195   \n",
      "19           0             0                0          0       0.250000   \n",
      "\n",
      "    nr_of_words  average_word_len  std_dev_word_len  min_word_len  \\\n",
      "0             4          6.000000          2.943920             2   \n",
      "1            13          5.076923          3.174417             2   \n",
      "2             2          4.500000          0.707107             4   \n",
      "3             3          3.333333          1.527525             2   \n",
      "4             6          4.833333          3.060501             2   \n",
      "5            10          4.200000          1.813529             2   \n",
      "6             2          5.000000          0.000000             5   \n",
      "7             2          7.000000          1.414214             6   \n",
      "8             4          7.500000          4.434712             2   \n",
      "9             1          7.000000          0.000000             7   \n",
      "10           12          4.583333          1.880925             2   \n",
      "11           10          4.100000          1.728840             2   \n",
      "12           10          4.500000          2.173067             1   \n",
      "13            8          5.625000          3.997767             2   \n",
      "14            4          4.250000          1.258306             3   \n",
      "15           18          4.222222          2.184317             1   \n",
      "16            7          4.571429          1.902379             2   \n",
      "17            3          3.333333          0.577350             3   \n",
      "18           18          3.500000          2.007339             1   \n",
      "19            5          3.800000          1.788854             1   \n",
      "\n",
      "    max_word_len  time  weekday  \n",
      "0              9  1192        3  \n",
      "1             11   518        2  \n",
      "2              5   172        2  \n",
      "3              5   164        2  \n",
      "4             10   322        1  \n",
      "5              7    60        0  \n",
      "6              5   237        4  \n",
      "7              8   571        0  \n",
      "8             12   211        6  \n",
      "9              7   244        1  \n",
      "10             8  1235        6  \n",
      "11             8   345        6  \n",
      "12             8   986        5  \n",
      "13            14  1122        4  \n",
      "14             6   142        4  \n",
      "15             9  1385        2  \n",
      "16             7  1362        2  \n",
      "17             4   504        4  \n",
      "18             8   543        2  \n",
      "19             5   155        2  \n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "### CREATING DATAFRAME WITH FEATURES ###\n",
    "########################################\n",
    "\n",
    "feature_df = transform(tweets_raw)\n",
    "print(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a file, where you ant to store the data\n",
    "file = open('../data/tweets_features', 'wb')\n",
    "# dump information to that file\n",
    "pickle.dump(feature_df, file)\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-eHvJNATwnXU"
   },
   "source": [
    "testy transform_row function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "L1vOtZJgwkWG",
    "outputId": "b58b323e-3e68-4fae-a9d3-1df731bb606f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è https://t.co/0shuUYUBEv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['katyperry',\n",
       " [13, 35, 36, 37, 2, 38, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [2, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [17,\n",
       "  8,\n",
       "  17,\n",
       "  18,\n",
       "  4,\n",
       "  16,\n",
       "  17,\n",
       "  0,\n",
       "  16,\n",
       "  4,\n",
       "  3,\n",
       "  14,\n",
       "  8,\n",
       "  13,\n",
       "  8,\n",
       "  18,\n",
       "  5,\n",
       "  14,\n",
       "  16,\n",
       "  18,\n",
       "  7,\n",
       "  4,\n",
       "  12,\n",
       "  17,\n",
       "  4,\n",
       "  11,\n",
       "  20,\n",
       "  4,\n",
       "  17],\n",
       " 67,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0.8055555555555556,\n",
       " 6,\n",
       " 4.833333333333333,\n",
       " 3.0605010483034745,\n",
       " 2,\n",
       " 10,\n",
       " 322,\n",
       " 1]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "which_tweet = 4\n",
    "tweet = tweets_raw['content'][which_tweet]\n",
    "author = tweets_raw['author'][which_tweet]\n",
    "nr_of_shares = tweets_raw['number_of_shares'][which_tweet]\n",
    "nr_of_likes = tweets_raw['number_of_likes'][which_tweet]\n",
    "date_time = tweets_raw['date_time'][which_tweet]\n",
    "print(tweet)\n",
    "tokenizer_long = create_tokenizer(tweets_raw['content'][:20], \"long\")\n",
    "tokenizer_short = create_tokenizer(tweets_raw['content'][:20], \"short\")\n",
    "char_codes = create_char_codes(tweets_raw['content'][:20], \"long\")\n",
    "max_words_nr_long = get_max_words_nr(tweets_raw['content'][:20], \"long\")\n",
    "max_words_nr_short = get_max_words_nr(tweets_raw['content'][:20], \"short\")\n",
    "#print(tokenizer_long.word_index)\n",
    "#print(tokenizer_short.word_index)\n",
    "transform_row(tweet, author, nr_of_shares, nr_of_likes, date_time, tokenizer_long, tokenizer_short, char_codes, max_words_nr_long, max_words_nr_long, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EN8b_KzPtY3w"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tweets preprocessing and feature extraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
